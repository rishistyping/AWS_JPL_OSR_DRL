# Increase this as episodes become more similar and batches won't have as much variance
batch_size="128"
# Decay this to promote convergence
beta_entropy="0.001"
discount_factor="0.995"
e_greedy_value="1.0"
epsilon_steps="10000"
exploration_type="categorical"
loss_type="huber"
# Decay this on a linear schedule, according to Unity-ML
lr="0.0001"
lr_decay_rate="0"
lr_decay_steps="0"
# Increase this over time, to allow selection of "novel" episodes
num_episodes_between_training="30"
# Decrease this over time, also to help avid NaN issues
num_epochs="8"
stack_size="1"
term_cond_avg_score="100000.0"
term_cond_max_episodes="100000"
aws_region="us-east-1"
stack_size="1"
job_duration="14400"
world_name="Canada_Training"
#change_start_position="false"
# This needs to be < 1.0, it's a PERCENTAGE
#start_position="0.96"
# Use the JOB_ID here if we are retraining
pretrained_id="20191018120257-b39fcdef-bcf7-48bc-bde8-608d42d7daf5"
